

-Huge footer.  Links to courses, social media for me, etc.  Any link I can think of.
-It's okay to have a box of text with only a shadow, no border (or nav bar)
-Black text not black.  Coursera had about a #252525
-Main page felt flowy and almost mobile design because it was all centered
	with a good degree of margin on the sides (10%-15% each side)
-Pack things a bit tight so it feels full and competant
-Some nav links on the top left, then log in button and sign in button to the right
-Less obvious and sharp borders and markers are better (shadows, clean color lines with no border, etc)



Experience notes:
-I want the user to be automatically redirected to the login page at times, but for it to automatically take them to their
	original desired destination when it is done processing their new/existing account.
-No blank account page for when not logged in.  redirect.
-Progress page either different or non-existant if not logged in.  Redirect or see promo about progress possibilities.



Pages:
-index
-contact
-topics
	-page for each topic's lessons (each lesson page has side bar nav for points in that lesson) (side nav with colored banner top)
-account pages
	-sign in
	-sign up
	-log out
	-see acount
-my progress
	-chosen topics to learn, progress in each



Things for the index page
-nav (sign-in/up, topics page button that also drops down, contact/suggestions, account if signed in, my progress)
-Title
-Subtext describing site
-"Lessons at every level"
-Button for topics page ||OR|| signup button if not signed in
-Background for why this page
-Image of me?  OR of something else
-Progress tracking page promo, link to it
-Final logo again and link to go to topics page
-Footer with all kinds of links.  Link to each Lesson/Topic?



------------------------------[ HTML and PHP layout end here ]--------------------------------------
-------------------------[ Topics and Content make up rest of file ]--------------------------------
Thoughts on Items:
I don't think we need a section on error functions. They might not make sense until the model itself is learned,
and if there was a section, they would basically need to be cut up into sections based on what kind of models they
were usable in anyways.  Maybe a cheat-sheet later.

Topics:
Each topic has points, each of which are lessons.  A lesson has multiple pages of content that is
made up of text explainations, videos, images, quizes, and possibly some interactive stuff too.
So, it is: Topic -> Lessons -> Pages
And pages might and might not be utilized. MAybe later for the more complicated topics





Background Mathmatics
-Machine Learning overview (what it is and isn't)
-Derivatives / basic calculus (Short of gradient descent)
-matrices and their operations


General Learning Things
-Gradient Descent - what is it and how is it done? (Vanilla)
-Hyperparameters and how to use / tune them
-	<Then do model topics?>
-Evaluating a model (accuracy || loss/error || generalization)
-Evaluation continued (conf matrix || roc curve)
-Comparing models (compare stats || roc curve comparison)


Linear Regression
-General goal of model && terms defined
-Hyperparameters and Learned values
-Gradient Descent - How it looks and the Math behind it (least square error, then descent)
-Problems and Sticking points


Logistic Regression
-General goal of model && terms defined
-Hyperparameters and Learned values
-Gradient Descent - How it looks and the Math behind it (Binary cross entropy loss)
-Problems and Sticking points


Multiple / Multinomial Linear and Logistic Regression
-Expanding our previous models - how
-Matrices at work (multiple gradients || different operations and input)
-Implications of high dimensionality in our learned values
-Visualizing Multivariate Descent (Hopefully find a way to show the implications of multivariate descent on a single axis dimension (bouncing around))


Different methods of Gradient Descent
-Vanilla and it's pitfalls (teaser for visualizing loss functions in high dimensionalities)
-Stochastic - What is different about it and it's pros and cons
-Adagrad - what it does and how that helps
-Momentum - what it does and how it helps
-rmsProp - How it is different from Adagrad and why that is a good thing
-Adam - How it puts all these together and why it works well


Decision Trees
-What it is, how it is structured, and how it classifies
-Training (or growing) a tree
-pros and cons to decision trees (tease for ensambles and forests)


Vanilla Neural Networks
-Feed Forward - intro, intuition and math
-Back Prop - intuition and math
-Mathmatical background on how it works (more like Killian's, with his example of lines and funcs)
-Derivatives in a Neural Network (visual explaination, then a mathmatical one)
-Hyperparameters of a Neural Network (layers || nodes per each layer || batch_size || epoch_count)





Future content:
support vector machines
ensambles  (random forest || bagging)
visualizing high-dimensional loss functions
skip connections in neural nets
CNNs (how convolutions work and it's derivatives)
RNNs (Why feeding previous result back in is powerful) (Vanilla)
LSTMs (how they work and how they help)













Lesson Segments:
I got carried away writing for the gradient descent segment, and
got this that might be useful when working on the Calculus section.
				<p>
					The important part to understand as fully as possible is that a function's derivative at a given point is
					literally just the slope of the function at that point.  Usually you can find this easily enough by taking
					another point close by, drawing a line between the two, and assume that it's close enough.  But by knowing
					the derivative function of the original function, you can find that line without having to know anything
					about the points around you!
				</p>
				<p>
					So why does this matter?
				</p>
				<p>
					Knowing the slope of a function at a given point can be very powerful.  Let's say that instead of an arbitrary
					function like in the image above, the function showed the selling price of a house, and the amount of money put
					into repairs for that house.  If you had this chart, and you were repairing a house with the idea of selling it,
					you wouldn't want to put money into the house, then pay a few hundred dollars to have it re-appraised to see if
					the repair was, in fact, worth it.
				</p>
				<p>
					Knowing the derivative, in this case, would 
					
					
					
Next bits for Graident Descent:
-Mathmatical definition of gradient, weight, and change.  Then add in learn rate
-(LR is tease for Hyperparameters (next time lesson) ).
-Gradient of weight = derivative of error with respect to weight
-